import os
import time
import asyncio
from datetime import datetime
from typing import Dict, Any
from bson import ObjectId

from models.database import db
from models.video_job import VideoJob, TranscriptSegment, Topic, Frame, SubTopic
from services.drive_service import drive_service
from services.youtube_service import youtube_service
from services.playwright_youtube_service import playwright_youtube_service
from services.gemini_service import gemini_service
from utils.ffmpeg_utils import FFmpegUtils
from utils.roi_utils import merge_time_windows
from utils.image_processing import ImageProcessor
import config


class ProcessingPipeline:
    """Orchestrates the video processing pipeline"""
    
    def __init__(self):
        self.ffmpeg = FFmpegUtils()
    
    async def process_video(self, job_id: str, playlist_context: str = None):
        """
        Main processing pipeline
        
        Steps:
        1. Download video from Drive
        2. Extract audio and frames
        3. Transcribe audio (chunked)
        4. Analyze transcript
        5. Analyze frames
        6. Synthesize results
        7. Store in MongoDB and Drive
        """
        try:
            # Get job from database
            job = await self._get_job(job_id)
            
            # Determine video source
            video_source = job.get("video_source", "drive")
            if not video_source:
                if job.get("youtube_url"):
                    video_source = "youtube"
                elif job.get("uploaded_video_path"):
                    video_source = "upload"
                elif job.get("drive_video_url"):
                    video_source = "drive"
            
            # Update status
            await self._update_job(job_id, {
                "status": "downloading",
                "progress": 0.05,
                "video_source": video_source,
                "message": f"Connecting to {video_source} to download your video..."
            })
            
            # Step 1: Download video based on source
            video_path = await self._download_video(job, video_source)
            await self._update_job(job_id, {
                "progress": 0.1,
                "message": "Video secured! Now preparing for detailed analysis..."
            })
            
            # Get video metadata
            duration = self.ffmpeg.get_video_duration(video_path)
            await self._update_job(job_id, {"duration": duration})
            
            # Step 2: Extract audio
            await self._update_job(job_id, {
                "status": "extracting",
                "progress": 0.15,
                "message": "Extracting high-quality audio for transcription..."
            })
            audio_path = await self._extract_audio(video_path, job_id)
            await self._update_job(job_id, {
                "progress": 0.25,
                "message": "Audio ready. Starting AI transcription engine..."
            })
            
            # Step 3: Transcribe audio
            await self._update_job(job_id, {
                "status": "transcribing",
                "progress": 0.3,
                "message": "The AI is transcribing the audio and identifying speakers..."
            })
            transcript = await self._transcribe_audio(audio_path)
            await self._update_job(job_id, {
                "transcript": [seg.model_dump() for seg in transcript],
                "progress": 0.5,
                "message": "Transcription complete. Detecting visual cues and landmarks..."
            })
            
            # NEW: Phase 1 - Audio Cue Scout
            print("Running Audio Cue Scout...")
            audio_cues = await gemini_service.detect_transcript_visual_cues(transcript)
            print(f"Detected {len(audio_cues)} audio cues")
            await self._update_job(job_id, {"audio_visual_cues": audio_cues})
            
            # Step 4: Analyze transcript
            await self._update_job(job_id, {
                "status": "analyzing",
                "progress": 0.55,
                "message": "Analyzing the transcript to identify key topics and segments..."
            })
            
            # Build transcript with timestamp context for better analysis
            transcript_segments_with_time = []
            for seg in transcript:
                start_ts = self.ffmpeg.format_timestamp(seg.start_time)
                end_ts = self.ffmpeg.format_timestamp(seg.end_time)
                transcript_segments_with_time.append(f"[{start_ts}-{end_ts}] {seg.text}")
            
            transcript_text = " ".join([seg.text for seg in transcript])
            
            # Log transcript coverage for debugging
            if transcript:
                first_seg_time = transcript[0].start_time
                last_seg_time = max(seg.end_time for seg in transcript)
                print(f"Transcript covers: {self.ffmpeg.format_timestamp(first_seg_time)} to {self.ffmpeg.format_timestamp(last_seg_time)} (video duration: {self.ffmpeg.format_timestamp(duration)})")
                print(f"Total transcript segments: {len(transcript)}, Total characters: {len(transcript_text)}")

            # Step 4a: Classify video genre (to adapt downstream prompting)
            genre_info = await gemini_service.classify_video_genre(transcript_text, duration)
            video_genre = genre_info.get("genre", "unknown")
            genre_confidence = genre_info.get("confidence", 0.0)
            genre_reason = genre_info.get("reason", "")
            print(f"Detected genre: {video_genre} (confidence={genre_confidence})")
            await self._update_job(job_id, {
                "video_genre": video_genre,
                "genre_confidence": genre_confidence,
                "genre_reason": genre_reason,
            })
            
            transcript_analysis = await gemini_service.analyze_transcript(
                transcript_text, 
                duration,
                video_genre=video_genre,
                playlist_context=playlist_context
            )
            
            # Filter out ads/sponsorships
            original_topics = transcript_analysis.get("topics", [])
            filtered_topics = [
                t for t in original_topics 
                if t.get("type", "content") != "ad" and "sponsor" not in t.get("title", "").lower()
            ]
            if len(filtered_topics) < len(original_topics):
                print(f"Filtered out {len(original_topics) - len(filtered_topics)} ad/sponsorship topics.")
                transcript_analysis["topics"] = filtered_topics
                
            await self._update_job(job_id, {
                "progress": 0.6,
                "message": "Filtering transcript for relevance andremoving distractions..."
            })
            
            # Step 5: Extract frames
            await self._update_job(job_id, {
                "progress": 0.65,
                "message": "Scanning video frames to identify the most important visual moments..."
            })
            # Phase 1: Coarse Visual Sampling (every 30s)
            raw_frames = await self._extract_frames(video_path, job_id, transcript_analysis, interval=30)
            
            # NEW: Phase 1 - Visual Gatekeeper (parallel evaluation)
            print(f"⚡ Running Visual Gatekeeper on {len(raw_frames)} frames in parallel...")
            useful_frames = []
            visual_rois = []
            
            # Semaphore to limit concurrent Gemini Vision calls (tuned in config.py)
            gate_sem = asyncio.Semaphore(config.MAX_CONCURRENT_VISION_TASKS)
            
            async def evaluate_single_frame(index, frame_path, timestamp):
                """Evaluate a single frame with concurrency control"""
                async with gate_sem:
                    evaluation = await gemini_service.evaluate_frame_content(frame_path)
                    return index, frame_path, timestamp, evaluation
            
            # Launch all frame evaluations in parallel
            gate_tasks = [
                evaluate_single_frame(i, fp, ts)
                for i, (fp, ts) in enumerate(raw_frames)
            ]
            gate_results = await asyncio.gather(*gate_tasks, return_exceptions=True)
            
            # Process results in original order
            for result in sorted(gate_results, key=lambda x: x[0] if not isinstance(x, Exception) else float('inf')):
                if isinstance(result, Exception):
                    print(f"  Frame evaluation error: {result}")
                    continue
                    
                i, frame_path, timestamp, evaluation = result
                is_useful = evaluation.get("is_useful", False)
                category = evaluation.get("category", "unknown")
                
                visual_rois.append({
                    "timestamp": timestamp,
                    "timestamp_str": self.ffmpeg.format_timestamp(timestamp),
                    "frame_path": frame_path,
                    "evaluation": evaluation
                })
                
                if is_useful:
                    useful_frames.append((frame_path, timestamp))
                    print(f"  Frame {i} at {timestamp}s: KEPT ({category})")
                else:
                    print(f"  Frame {i} at {timestamp}s: DROPPED ({category})")
            
            print(f"Gatekeeper: Kept {len(useful_frames)}/{len(raw_frames)} frames")

            
            # NEW: Phase 2 - ROI Fusion & Dense Sampling
            print("Merging ROIs for Phase 2 processing...")
            processing_windows = merge_time_windows(
                audio_cues, 
                visual_rois, 
                duration,
                buffer_seconds=5.0, # 5s buffer around events
                min_gap=5.0 # Merge if gaps < 5s
            )
            print(f"Identified {len(processing_windows)} processing windows for dense sampling.")
            
            # Extract high-frequency frames only in these windows
            frames_dir = os.path.join(config.TEMP_DIR, f"{job_id}_frames")
            dense_frames = []
            
            if processing_windows:
                dense_frames = self.ffmpeg.extract_dense_frames(
                    video_path,
                    frames_dir,
                    processing_windows,
                    fps=1
                )
                print(f"Extracted {len(dense_frames)} additional dense frames.")

            await self._update_job(job_id, {
                "visual_rois": visual_rois,
                "total_frames_extracted": len(raw_frames),
                "useful_frames_count": len(useful_frames),
                "processing_windows": processing_windows,
                "dense_frames_count": len(dense_frames),
                "progress": 0.7,
                "message": "Visual landmarks detected. Deduplicating and selecting 'hero' frames..."
            })
            
            # Combine useful coarse frames with dense frames
            # Use a dictionary to de-duplicate by timestamp (rounded to nearest second)
            combined_frames_map = {}
            
            # Add coarse frames first
            for path, ts in useful_frames:
                combined_frames_map[int(ts)] = (path, ts)
                
            # Add/Overwrite with dense frames (prefer dense as they are fresher?)
            # Actually, both are fine, but dense frames are 1fps in active regions.
            for path, ts in dense_frames:
                combined_frames_map[int(ts)] = (path, ts)
            
            # Sorted list of unique frames
            frames = sorted(combined_frames_map.values(), key=lambda x: x[1])
            print(f"Total unique frames for visual processing: {len(frames)}")

            # Phase 3: Visual Intelligence (The "Clean Up")
            # Step 1: Visual Deduplication
            print(f"Clustering {len(frames)} frames to find unique visual topics...")
            # Threshold 12 is a reasonable starting point for 64-bit dHash (0-64 distance)
            clusters = ImageProcessor.cluster_frames(frames, threshold=12)
            print(f"Found {len(clusters)} unique visual clusters/slides.")
            
            await self._update_job(job_id, {
                "progress": 0.75,
                "visual_clusters_count": len(clusters),
                "visual_clusters_preview": [{"start": c["start_time"], "end": c["end_time"], "count": c["frame_count"]} for c in clusters[:10]],
                "message": "Generating visual sub-topics and cross-referencing with audio..."
            })
            
            # Step 2: Hero Frame Selector
            print(f"Selecting Hero Frames for {len(clusters)} clusters...")
            visual_subtopics = await gemini_service.analyze_frame_clusters(clusters)
            
            # Step 3: Upload Hero Frames & Map to frame_analyses
            print("Uploading Hero Frames to Google Drive...")
            
            # Create folder in Drive for this job
            folder_name = f"video_{job_id}_frames"
            try:
                # Reuse existing folder logic if possible or create new
                folder_id = drive_service.create_folder(
                    folder_name,
                    parent_folder_id=config.DRIVE_FOLDER_ID
                )
                await self._update_job(job_id, {
                    "drive_folder_id": folder_id,
                    "message": "Uploading key visual frames to Google Drive for your report..."
                })
            except Exception as e:
                print(f"Error creating Drive folder: {e}")
                folder_id = config.DRIVE_FOLDER_ID # Fallback
            
            # We map "visual_subtopics" (Phase 3 result) to "frame_analyses" (Phase 1 structure)
            # Upload hero frames in parallel for speed
            print(f"⚡ Uploading {len(visual_subtopics)} hero frames to Drive in parallel...")
            upload_sem = asyncio.Semaphore(config.MAX_CONCURRENT_UPLOADS)
            
            async def upload_single_frame(index, item):
                """Upload a single hero frame to Drive with concurrency control"""
                frame_path = item["hero_frame_path"]
                timestamp = item["timestamp"]
                drive_url = None
                
                async with upload_sem:
                    try:
                        if os.path.exists(frame_path):
                            # Run sync Drive upload in a thread to avoid blocking
                            uploaded = await asyncio.to_thread(
                                drive_service.upload_file,
                                frame_path,
                                folder_id,
                                f"hero_{index:02d}_{int(timestamp)}s.jpg"
                            )
                            file_id = uploaded.get("id")
                            drive_url = f"https://drive.google.com/thumbnail?id={file_id}&sz=w800"
                    except Exception as e:
                        print(f"Error uploading frame {frame_path}: {e}")
                
                return index, {
                    "frame_path": frame_path,
                    "drive_url": drive_url,
                    "timestamp": timestamp,
                    "description": item["sub_topic_title"],
                    "ocr_text": " ".join(item.get("keywords", [])),
                    "type": "slide",
                    "insights": item["visual_summary"]
                }
            
            upload_tasks = [
                upload_single_frame(i, item)
                for i, item in enumerate(visual_subtopics)
            ]
            upload_results = await asyncio.gather(*upload_tasks, return_exceptions=True)
            
            # Collect results in order
            frame_analyses = []
            for result in sorted(upload_results, key=lambda x: x[0] if not isinstance(x, Exception) else float('inf')):
                if isinstance(result, Exception):
                    print(f"  Upload error: {result}")
                    continue
                _, analysis = result
                frame_analyses.append(analysis)

            
            await self._update_job(job_id, {
                "progress": 0.85,
                "visual_subtopics": visual_subtopics,
                "message": "Almost there! Combining all insights into your final structured report..."
            })
            
            # Step 7: Synthesize results
            await self._update_job(job_id, {
                "status": "synthesizing",
                "progress": 0.9,
                "message": "AI is generating your executive summary and key takeaways..."
            })
            synthesis = await gemini_service.synthesize_results(
                transcript_analysis,
                frame_analyses,
                duration,
                video_genre=video_genre,
                playlist_context=playlist_context
            )
            
            # CRITICAL: Post-synthesis ad cleanup
            # Sometimes synthesis re-introduces ad topics if they are prominent
            original_synth_topics = synthesis.get("topics", [])
            clean_synth_topics = [
                t for t in original_synth_topics 
                if "sponsor" not in t.get("title", "").lower() and t.get("type", "content") != "ad"
            ]
            if len(clean_synth_topics) < len(original_synth_topics):
                 print(f"Filtered {len(original_synth_topics) - len(clean_synth_topics)} ad topics from synthesis result.")
                 synthesis["topics"] = clean_synth_topics
            
            # Step 7.1: Map Visuals to Topics (Phase 4)
            print("Mapping visual sub-topics to synthesized main topics...")
            main_topics = synthesis.get("topics", [])
            mapped_topics = await gemini_service.map_visuals_to_topics(main_topics, visual_subtopics)
            
            # Update synthesis with mapped topics
            synthesis["topics"] = mapped_topics
            
            # Step 7.2: Generate 5-Slide Executive Summary
            print("Generating 5-slide executive summary...")
            slide_summary = []
            try:
                await self._update_job(job_id, {
                    "progress": 0.95,
                    "message": "Generating a 5-slide executive presentation for you..."
                })
                slide_summary = await gemini_service.generate_slide_summary(
                    transcript_text=transcript_text,
                    executive_summary=synthesis.get("executive_summary", ""),
                    key_takeaways=synthesis.get("key_takeaways", []),
                    topics=synthesis.get("topics", []),
                    duration=duration,
                    video_genre=video_genre
                )
                print(f"Generated {len(slide_summary)} slides.")
            except Exception as e:
                print(f"Slide summary generation failed (non-blocking): {e}")
            
            # Step 8: Build final output
            topics = await self._build_topics(
                synthesis.get("topics", []),
                frame_analyses,
                transcript
            )
            
            # Step 9: Update job with final results
            # Convert to dict - topics are Pydantic models, frames are already dicts
            topics_data = [topic.model_dump() for topic in topics]
            frames_data = frame_analyses if frame_analyses else []
            
            await self._update_job(job_id, {
                "status": "completed",
                "progress": 1.0,
                "topics": topics_data,
                "frames": frames_data,
                "executive_summary": synthesis.get("executive_summary", ""),
                "key_takeaways": synthesis.get("key_takeaways", []),
                "entities": synthesis.get("entities", {}),
                "slide_summary": slide_summary,
                "report": synthesis,  # Store full synthesis result
                "video_genre": video_genre,
                "genre_confidence": genre_confidence,
                "completed_at": datetime.utcnow(),
                "message": "Report complete! Rendering your insights now."
            })
            
            # Cleanup temp files
            await self._cleanup(job_id)
            
            print(f"Job {job_id} completed successfully")
            
        except Exception as e:
            print(f"Error processing job {job_id}: {e}")
            await self._update_job(job_id, {
                "status": "failed",
                "error_message": str(e)
            })
    
    async def _download_video(self, job: Dict, video_source: str = "drive") -> str:
        """Download video from Google Drive, YouTube, or use uploaded file"""
        
        if video_source == "upload" or job.get("uploaded_video_path"):
            # Use uploaded file
            video_path = job.get("uploaded_video_path")
            if not video_path or not os.path.exists(video_path):
                raise Exception(f"Uploaded video file not found at {video_path}")
            
            # Update video name if not set
            if not job.get("video_name"):
                filename = os.path.basename(video_path)
                await self._update_job(str(job["_id"]), {
                    "video_name": filename
                })
            
            return video_path
        
        video_path = os.path.join(config.TEMP_DIR, f"{job['_id']}_video.mp4")
        
        if video_source == "youtube" or job.get("youtube_url"):
            # Download from YouTube
            youtube_url = job.get("youtube_url")
            if not youtube_url:
                raise Exception("YouTube URL not found in job")
            
            video_id = youtube_service.extract_video_id(youtube_url)
            
            # Get video metadata
            try:
                video_info = youtube_service.get_video_info(video_id)
                video_name = video_info.get("title", f"video_{job['_id']}.mp4")
                duration = video_info.get("duration", 0)
            except Exception as e:
                print(f"Warning: Could not fetch YouTube metadata: {e}")
                video_name = job.get("video_name") or f"video_{job['_id']}.mp4"
                duration = None
            
            # Update job with video info
            await self._update_job(str(job["_id"]), {
                "youtube_video_id": video_id,
                "video_name": video_name,
                "video_source": "youtube"
            })
            
            # Download video using yt-dlp + PO Token server (most reliable on Render)
            try:
                print(f"Downloading YouTube video {video_id} using yt-dlp strategy...")
                youtube_service.download_video(youtube_url, video_path, video_id)
            except Exception as e:
                print(f"yt-dlp download failed: {e}")
                print("Falling back to Playwright (may fail on Render)...")
                try:
                    await playwright_youtube_service.download_video(youtube_url, video_path)
                except Exception as pe:
                    print(f"Playwright fallback also failed: {pe}")
                    raise Exception(f"Failed to download YouTube video: {e}")
            
        else:
            # Download from Google Drive (existing logic)
            drive_url = job.get("drive_video_url")
            if not drive_url:
                raise Exception("Drive video URL not found in job")
            
            file_id = drive_service.extract_file_id(drive_url)
            
            # Get file metadata
            metadata = drive_service.get_file_metadata(file_id)
            video_name = metadata.get("name", f"video_{job['_id']}.mp4")
            
            # Update job with file info
            await self._update_job(str(job["_id"]), {
                "drive_file_id": file_id,
                "video_name": video_name,
                "video_source": "drive"
            })
            
            # Download to temp directory
            drive_service.download_file(file_id, video_path)
        
        return video_path
    
    async def _extract_audio(self, video_path: str, job_id: str) -> str:
        """Extract audio from video"""
        audio_path = os.path.join(config.TEMP_DIR, f"{job_id}_audio.wav")
        self.ffmpeg.extract_audio(video_path, audio_path)
        
        # Store audio path in database for downloads
        await self._update_job(job_id, {
            "audio_path": audio_path
        })
        
        return audio_path
    
    async def _transcribe_audio(self, audio_path: str) -> list[TranscriptSegment]:
        """Transcribe audio in chunks — runs chunks in parallel for speed"""
        # Split audio into chunks
        chunks = self.ffmpeg.split_audio(audio_path)
        
        # Semaphore to limit concurrent Gemini API calls (tuned in config.py)
        sem = asyncio.Semaphore(config.MAX_CONCURRENT_TRANSCRIBES)
        
        async def transcribe_chunk(chunk_path, start_time):
            """Transcribe a single chunk with concurrency control"""
            async with sem:
                try:
                    segments = await gemini_service.transcribe_audio(
                        chunk_path, 
                        start_time
                    )
                    return segments
                except Exception as e:
                    print(f"Error transcribing chunk {chunk_path}: {e}")
                    return []
        
        # Launch all chunks in parallel
        print(f"⚡ Transcribing {len(chunks)} audio chunks in parallel...")
        tasks = [
            transcribe_chunk(chunk_path, start_time)
            for chunk_path, start_time, end_time in chunks
        ]
        results = await asyncio.gather(*tasks)
        
        # Flatten results (maintain order since asyncio.gather preserves input order)
        all_segments = []
        for segments in results:
            all_segments.extend(segments)
        
        # Clean up all chunk files after parallel processing is done
        for chunk_path, _, _ in chunks:
            try:
                if os.path.exists(chunk_path):
                    import gc
                    gc.collect()
                    for attempt in range(5):
                        try:
                            time.sleep(0.3)
                            os.remove(chunk_path)
                            break
                        except (PermissionError, OSError):
                            if attempt < 4:
                                time.sleep(0.5)
                            else:
                                print(f"Warning: Could not delete {chunk_path} after retries, skipping")
            except Exception as cleanup_err:
                print(f"Warning: Chunk cleanup error (non-fatal): {cleanup_err}")
        
        # Deduplicate overlapping segments
        deduplicated = self._deduplicate_segments(all_segments)
        
        return deduplicated
    
    def _deduplicate_segments(
        self, 
        segments: list[TranscriptSegment]
    ) -> list[TranscriptSegment]:
        """Remove duplicate segments from overlapping chunks"""
        if not segments:
            return []
        
        # Sort by start time
        sorted_segments = sorted(segments, key=lambda x: x.start_time)
        
        deduplicated = [sorted_segments[0]]
        
        for seg in sorted_segments[1:]:
            last_seg = deduplicated[-1]
            
            # If this segment overlaps significantly (>70% overlap), merge or skip
            overlap_start = max(seg.start_time, last_seg.start_time)
            overlap_end = min(seg.end_time, last_seg.end_time)
            overlap_duration = max(0, overlap_end - overlap_start)
            
            last_duration = last_seg.end_time - last_seg.start_time
            seg_duration = seg.end_time - seg.start_time
            
            # If high overlap (>70% of either segment), prefer the one with more text or merge them
            if last_duration > 0 and overlap_duration / last_duration > 0.7:
                # Merge: keep the longer segment or the one with more text
                if len(seg.text) > len(last_seg.text) or seg_duration > last_duration:
                    deduplicated[-1] = seg
                continue
            elif seg_duration > 0 and overlap_duration / seg_duration > 0.7:
                # Same check from other direction
                if len(seg.text) > len(last_seg.text) or seg_duration > last_duration:
                    deduplicated[-1] = seg
                continue
            
            # If segments are very close (< 2 seconds gap), merge them
            gap = seg.start_time - last_seg.end_time
            if gap < 2 and gap > -2:  # Very close segments, merge
                # Merge text and extend end time
                merged_text = last_seg.text + " " + seg.text
                deduplicated[-1] = TranscriptSegment(
                    text=merged_text,
                    start_time=last_seg.start_time,
                    end_time=max(last_seg.end_time, seg.end_time),
                    speaker=last_seg.speaker or seg.speaker
                )
                continue
            
            deduplicated.append(seg)
        
        return deduplicated
    
    async def _extract_frames(
        self,
        video_path: str,
        job_id: str,
        transcript_analysis: Dict,
        interval: int = config.KEYFRAME_INTERVAL
    ) -> list[tuple[str, float]]:
        """Extract keyframes from video"""
        frames_dir = os.path.join(config.TEMP_DIR, f"{job_id}_frames")
        
        # Extract frames at regular intervals
        frames = self.ffmpeg.extract_keyframes(
            video_path,
            frames_dir,
            interval=interval
        )
        
        # TODO: In Phase 2, add smart frame selection based on visual_cues
        # from transcript_analysis
        
        return frames
    
    async def _analyze_frames(
        self,
        frames: list[tuple[str, float]],
        job_id: str,
        transcript_analysis: Dict
    ) -> list[Dict]:
        """Analyze frames and upload to Drive"""
        # Create folder in Drive for this job
        folder_name = f"video_{job_id}_frames"
        folder_id = drive_service.create_folder(
            folder_name,
            parent_folder_id=config.DRIVE_FOLDER_ID
        )
        
        await self._update_job(job_id, {"drive_folder_id": folder_id})
        
        # Get frame paths
        frame_paths = [path for path, _ in frames]
        
        # Analyze frames with Gemini Vision
        analyses = await gemini_service.analyze_frames(frame_paths)
        
        # Upload frames to Drive and add URLs
        for i, (analysis, (frame_path, timestamp)) in enumerate(zip(analyses, frames)):
            try:
                # Upload to Drive
                uploaded = drive_service.upload_file(
                    frame_path,
                    folder_id=folder_id,
                    file_name=f"frame_{i:04d}_{int(timestamp)}s.jpg"
                )
                
                # Add Drive URL to analysis
                analysis["drive_url"] = uploaded.get("webViewLink")
                analysis["timestamp"] = timestamp
                analysis["timestamp_str"] = self.ffmpeg.format_timestamp(timestamp)
                
            except Exception as e:
                print(f"Error uploading frame {frame_path}: {e}")
        
        return analyses
    
    async def _build_topics(
        self,
        topic_data: list[Dict],
        frame_analyses: list[Dict],
        transcript: list[TranscriptSegment]
    ) -> list[Topic]:
        """Build Topic objects with frames"""
        from services.gemini_service import timestamp_to_seconds, seconds_to_timestamp
        
        topics = []
        
        for topic_info in topic_data:
            # Parse timestamp range - ensure we have valid timestamps
            ts_range = topic_info.get("timestamp_range", ["00:00:00", "00:00:00"])
            
            # Convert to seconds and back to ensure consistency
            if ts_range and len(ts_range) >= 2:
                start_seconds = timestamp_to_seconds(ts_range[0])
                end_seconds = timestamp_to_seconds(ts_range[1])
            else:
                start_seconds = 0.0
                end_seconds = 0.0
            
            # Ensure timestamps are valid
            if start_seconds < 0:
                start_seconds = 0.0
            if end_seconds < start_seconds:
                end_seconds = start_seconds + 600  # Default 10 min if end is before start
            
            # Find frames within this topic's time range
            topic_frames = []
            for analysis in frame_analyses:
                frame_ts = analysis.get("timestamp", 0)
                if isinstance(frame_ts, str):
                    frame_ts = timestamp_to_seconds(frame_ts)
                    
                if start_seconds <= frame_ts <= end_seconds:
                    frame = Frame(
                        timestamp=seconds_to_timestamp(frame_ts),
                        frame_number=len(topic_frames),
                        drive_url=analysis.get("drive_url"),
                        description=analysis.get("description"),
                        ocr_text=analysis.get("ocr_text"),
                        type=analysis.get("type", "other")
                    )
                    topic_frames.append(frame)
            
            # Sub-topics processing (Phase 4)
            sub_topics_instances = []
            for sub in topic_info.get("sub_topics", []): 
                # Find matching frame URL if not present
                img_url = sub.get("image_url")
                sub_ts = sub.get("frame_timestamp")
                matched_fa = None
                
                if not img_url and sub_ts is not None:
                    # Find closest frame in frame_analyses
                    closest_diff = 2.0
                    closest_frame = None
                    for fa in frame_analyses:
                        fa_ts = fa.get("timestamp", 0)
                        if isinstance(fa_ts, str): continue
                        diff = abs(fa_ts - float(sub_ts))
                        if diff < closest_diff:
                            closest_diff = diff
                            closest_frame = fa
                    
                    if closest_frame:
                         img_url = closest_frame.get("drive_url")
                         matched_fa = closest_frame
                elif img_url:
                    # Find the fa for this img_url to add to frames later
                    for fa in frame_analyses:
                        if fa.get("drive_url") == img_url:
                            matched_fa = fa
                            break
                
                # Add to sub_topics list
                sub_topics_instances.append(SubTopic(
                    title=sub.get("title", "Visual Topic"),
                    visual_summary=sub.get("visual_summary", ""),
                    timestamp=sub.get("timestamp", "00:00:00"),
                    image_url=img_url
                ))

                # ALSO: Ensure this frame is in the main topic_frames list for Topic Covered section
                if matched_fa:
                    # Check if already present to avoid duplicates
                    exists = False
                    for existing in topic_frames:
                        if existing.drive_url == matched_fa.get("drive_url"):
                            exists = True
                            break
                    
                    if not exists:
                        topic_frames.append(Frame(
                            timestamp=seconds_to_timestamp(matched_fa.get("timestamp", 0)),
                            frame_number=len(topic_frames),
                            drive_url=matched_fa.get("drive_url"),
                            description=matched_fa.get("description") or matched_fa.get("insights"),
                            ocr_text=matched_fa.get("ocr_text"),
                            type=matched_fa.get("type", "slide")
                        ))

            topic = Topic(
                title=topic_info.get("title", "Untitled"),
                timestamp_range=[seconds_to_timestamp(start_seconds), seconds_to_timestamp(end_seconds)],
                start_seconds=start_seconds,
                end_seconds=end_seconds,
                summary=topic_info.get("summary", ""),
                key_points=topic_info.get("key_points", []),
                frames=topic_frames,
                quotes=topic_info.get("quotes", []),
                visual_cues=topic_info.get("visual_cues", []),
                sub_topics=sub_topics_instances
            )
            topics.append(topic)
        
        return topics
    
    def _parse_timestamp(self, ts: str) -> float:
        """Convert HH:MM:SS to seconds"""
        from services.gemini_service import timestamp_to_seconds
        return timestamp_to_seconds(ts)
    
    async def _cleanup(self, job_id: str):
        """Clean up temporary files (but keep audio for downloads)"""
        # Don't delete audio file - keep it for downloads
        # Only clean up video and frames
        
        # Get job to check video source
        job = await self._get_job(job_id)
        video_source = job.get("video_source", "drive")
        
        # Only delete video if it's not an uploaded file (uploaded files should be kept until manually deleted)
        if video_source != "upload":
            patterns = [f"{job_id}_video.mp4"]
            for pattern in patterns:
                path = os.path.join(config.TEMP_DIR, pattern)
                if os.path.exists(path):
                    try:
                        os.remove(path)
                    except Exception as e:
                        print(f"Error removing {path}: {e}")
        else:
            # For uploaded files, keep the original video but could clean up later
            print(f"Keeping uploaded video file for job {job_id}")
        
        # Remove frames directory
        frames_dir = os.path.join(config.TEMP_DIR, f"{job_id}_frames")
        if os.path.exists(frames_dir):
            try:
                import shutil
                shutil.rmtree(frames_dir)
            except Exception as e:
                print(f"Error removing frames directory: {e}")
    
    async def _get_job(self, job_id: str) -> Dict:
        """Get job from database"""
        database = db.get_db()
        job = await database.video_jobs.find_one({"_id": ObjectId(job_id)})
        if not job:
            raise Exception(f"Job {job_id} not found")
        return job
    
    async def _update_job(self, job_id: str, updates: Dict):
        """Update job in database with real-time logging support"""
        database = db.get_db()
        
        # If a message is provided, add it to processing_logs and set as current_action
        if "message" in updates:
            msg = updates.pop("message")
            log_entry = {
                "message": msg,
                "timestamp": datetime.utcnow().isoformat()
            }
            await database.video_jobs.update_one(
                {"_id": ObjectId(job_id)},
                {
                    "$push": {"processing_logs": log_entry},
                    "$set": {"current_action": msg}
                }
            )
            
        updates["updated_at"] = datetime.utcnow()
        await database.video_jobs.update_one(
            {"_id": ObjectId(job_id)},
            {"$set": updates}
        )


# Singleton instance
pipeline = ProcessingPipeline()
